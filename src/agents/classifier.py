"""
Agente especializado en clasificaci√≥n de documentos por temas
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict, Counter
import re

from langchain.schema import Document
from core.llm_manager import llm_manager

logger = logging.getLogger(__name__)


class TopicClassifier:
    """Agente para clasificar documentos por temas y categor√≠as"""

    def __init__(self):
        self.predefined_categories = {
            "business": [
                "negocio",
                "empresa",
                "comercial",
                "ventas",
                "marketing",
                "strategy",
            ],
            "technical": [
                "t√©cnico",
                "technology",
                "sistema",
                "software",
                "desarrollo",
                "programming",
            ],
            "legal": [
                "legal",
                "ley",
                "contrato",
                "regulaci√≥n",
                "compliance",
                "derecho",
            ],
            "financial": [
                "financiero",
                "dinero",
                "presupuesto",
                "costo",
                "investment",
                "budget",
            ],
            "academic": [
                "acad√©mico",
                "investigaci√≥n",
                "estudio",
                "an√°lisis",
                "research",
                "university",
            ],
            "healthcare": [
                "salud",
                "m√©dico",
                "hospital",
                "patient",
                "treatment",
                "medicina",
            ],
            "education": [
                "educaci√≥n",
                "ense√±anza",
                "estudiante",
                "curso",
                "learning",
                "school",
            ],
            "government": [
                "gobierno",
                "p√∫blico",
                "pol√≠tica",
                "policy",
                "administrative",
                "state",
            ],
        }

    async def classify_documents_by_topics(self, documents: List[Document]) -> str:
        """Clasificar documentos por temas principales"""

        # Extraer textos y metadatos
        doc_texts = [
            (doc.page_content, doc.metadata.get("source_file", "unknown"))
            for doc in documents
        ]

        # Agrupar por archivo fuente
        files_content = defaultdict(list)
        for text, source in doc_texts:
            files_content[source].append(text)

        # Clasificar cada archivo
        file_classifications = {}
        for filename, chunks in files_content.items():
            combined_text = " ".join(chunks)
            classification = await self._classify_single_document(
                combined_text, filename
            )
            file_classifications[filename] = classification

        # Generar reporte de clasificaci√≥n
        return await self._generate_classification_report(file_classifications)

    async def _classify_single_document(
        self, content: str, filename: str
    ) -> Dict[str, Any]:
        """Clasificar un documento individual"""

        # Clasificaci√≥n por LLM
        llm_classification = await self._llm_topic_classification(content, filename)

        # Clasificaci√≥n por palabras clave
        keyword_classification = self._keyword_classification(content)

        # Extracci√≥n de temas espec√≠ficos
        specific_topics = await self._extract_specific_topics(content)

        return {
            "filename": filename,
            "llm_classification": llm_classification,
            "keyword_categories": keyword_classification,
            "specific_topics": specific_topics,
            "content_preview": content[:200] + "..." if len(content) > 200 else content,
        }

    async def _llm_topic_classification(
        self, content: str, filename: str
    ) -> Dict[str, Any]:
        """Clasificaci√≥n usando LLM"""

        prompt = f"""
        Analiza el siguiente documento y clasif√≠calo por temas y categor√≠as:
        
        **Documento**: {filename}
        **Contenido**: {content[:1500]}...
        
        Proporciona la clasificaci√≥n en el siguiente formato:
        
        ## CATEGOR√çA PRINCIPAL
        [Una categor√≠a principal: Business, Technical, Legal, Financial, Academic, Healthcare, Education, Government, u Otro]
        
        ## TEMAS ESPEC√çFICOS
        [Lista de 3-5 temas espec√≠ficos identificados]
        
        ## NIVEL DE ESPECIALIZACI√ìN
        [General, Intermedio, o Avanzado]
        
        ## AUDIENCIA OBJETIVO
        [Descripci√≥n de la audiencia objetivo]
        
        ## PALABRAS CLAVE
        [5-8 palabras clave principales]
        
        Clasificaci√≥n:
        """

        response = llm_manager.generate_response(prompt)

        # Parsear respuesta
        return self._parse_llm_classification(response)

    def _parse_llm_classification(self, llm_response: str) -> Dict[str, Any]:
        """Parsear respuesta del LLM para extraer clasificaci√≥n estructurada"""

        classification = {
            "primary_category": "Unknown",
            "specific_topics": [],
            "specialization_level": "General",
            "target_audience": "General",
            "keywords": [],
            "raw_response": llm_response,
        }

        # Extraer categor√≠a principal
        category_match = re.search(
            r"CATEGOR√çA PRINCIPAL\s*[:\-]?\s*([^\n]+)", llm_response, re.IGNORECASE
        )
        if category_match:
            classification["primary_category"] = category_match.group(1).strip()

        # Extraer temas espec√≠ficos
        topics_section = re.search(
            r"TEMAS ESPEC√çFICOS\s*[:\-]?(.*?)(?=##|$)",
            llm_response,
            re.IGNORECASE | re.DOTALL,
        )
        if topics_section:
            topics_text = topics_section.group(1)
            topics = [
                topic.strip().strip("‚Ä¢-*")
                for topic in topics_text.split("\n")
                if topic.strip()
            ]
            classification["specific_topics"] = [t for t in topics if len(t) > 3][:5]

        # Extraer nivel de especializaci√≥n
        level_match = re.search(
            r"NIVEL DE ESPECIALIZACI√ìN\s*[:\-]?\s*([^\n]+)", llm_response, re.IGNORECASE
        )
        if level_match:
            classification["specialization_level"] = level_match.group(1).strip()

        # Extraer audiencia objetivo
        audience_match = re.search(
            r"AUDIENCIA OBJETIVO\s*[:\-]?\s*([^\n]+)", llm_response, re.IGNORECASE
        )
        if audience_match:
            classification["target_audience"] = audience_match.group(1).strip()

        # Extraer palabras clave
        keywords_section = re.search(
            r"PALABRAS CLAVE\s*[:\-]?(.*?)(?=##|$)",
            llm_response,
            re.IGNORECASE | re.DOTALL,
        )
        if keywords_section:
            keywords_text = keywords_section.group(1)
            keywords = [
                kw.strip().strip("‚Ä¢-*,") for kw in keywords_text.split() if kw.strip()
            ]
            classification["keywords"] = keywords[:8]

        return classification

    def _keyword_classification(self, content: str) -> Dict[str, float]:
        """Clasificaci√≥n basada en palabras clave predefinidas"""

        content_lower = content.lower()
        category_scores = {}

        for category, keywords in self.predefined_categories.items():
            score = 0
            for keyword in keywords:
                score += content_lower.count(keyword.lower())

            # Normalizar por longitud del documento
            normalized_score = score / len(content.split()) * 1000
            category_scores[category] = normalized_score

        return category_scores

    async def _extract_specific_topics(self, content: str) -> List[str]:
        """Extraer temas espec√≠ficos del contenido"""

        prompt = f"""
        Extrae los temas espec√≠ficos m√°s importantes del siguiente texto:
        
        {content[:1000]}...
        
        Instrucciones:
        - Lista entre 3-7 temas espec√≠ficos
        - S√© preciso y descriptivo
        - Evita t√©rminos muy generales
        - Un tema por l√≠nea
        
        Temas espec√≠ficos:
        """

        response = llm_manager.generate_response(prompt)

        # Extraer temas de la respuesta
        topics = []
        for line in response.split("\n"):
            topic = line.strip().strip("‚Ä¢-*0123456789.")
            if len(topic) > 3 and topic not in topics:
                topics.append(topic)

        return topics[:7]

    async def _generate_classification_report(
        self, classifications: Dict[str, Dict[str, Any]]
    ) -> str:
        """Generar reporte consolidado de clasificaci√≥n"""

        # An√°lisis estad√≠stico
        all_categories = [
            c["llm_classification"]["primary_category"]
            for c in classifications.values()
        ]
        category_counts = Counter(all_categories)

        all_topics = []
        for c in classifications.values():
            all_topics.extend(c["llm_classification"]["specific_topics"])
        topic_counts = Counter(all_topics)

        # Construir reporte
        report_sections = []

        # Resumen ejecutivo
        report_sections.append("# üìä REPORTE DE CLASIFICACI√ìN DE DOCUMENTOS\n")
        report_sections.append(
            f"**Total de documentos analizados**: {len(classifications)}\n"
        )

        # Distribuci√≥n por categor√≠as
        report_sections.append("## üìà DISTRIBUCI√ìN POR CATEGOR√çAS\n")
        for category, count in category_counts.most_common():
            percentage = (count / len(classifications)) * 100
            report_sections.append(
                f"- **{category}**: {count} documentos ({percentage:.1f}%)"
            )

        # Temas m√°s frecuentes
        report_sections.append("\n## üîç TEMAS M√ÅS FRECUENTES\n")
        for topic, count in topic_counts.most_common(10):
            report_sections.append(f"- {topic} ({count} menciones)")

        # Clasificaci√≥n por documento
        report_sections.append("\n## üìÑ CLASIFICACI√ìN POR DOCUMENTO\n")
        for filename, classification in classifications.items():
            llm_class = classification["llm_classification"]
            report_sections.append(
                f"""
### üìã {filename}
- **Categor√≠a**: {llm_class["primary_category"]}
- **Nivel**: {llm_class["specialization_level"]}
- **Audiencia**: {llm_class["target_audience"]}
- **Temas**: {", ".join(llm_class["specific_topics"])}
- **Palabras clave**: {", ".join(llm_class["keywords"])}
            """
            )

        # An√°lisis de diversidad tem√°tica
        report_sections.append("\n## üåü AN√ÅLISIS DE DIVERSIDAD TEM√ÅTICA\n")

        diversity_analysis = await self._analyze_thematic_diversity(classifications)
        report_sections.append(diversity_analysis)

        return "\n".join(report_sections)

    async def _analyze_thematic_diversity(
        self, classifications: Dict[str, Dict[str, Any]]
    ) -> str:
        """Analizar diversidad tem√°tica del conjunto de documentos"""

        categories = [
            c["llm_classification"]["primary_category"]
            for c in classifications.values()
        ]
        unique_categories = len(set(categories))

        all_topics = []
        for c in classifications.values():
            all_topics.extend(c["llm_classification"]["specific_topics"])
        unique_topics = len(set(all_topics))

        # Calcular m√©tricas de diversidad
        category_diversity = unique_categories / len(categories) if categories else 0
        topic_diversity = unique_topics / len(all_topics) if all_topics else 0

        analysis = f"""
**Diversidad de categor√≠as**: {category_diversity:.2f} ({unique_categories} categor√≠as √∫nicas)
**Diversidad de temas**: {topic_diversity:.2f} ({unique_topics} temas √∫nicos)

**Interpretaci√≥n**:
"""

        if category_diversity > 0.7:
            analysis += "- ‚úÖ Alta diversidad categ√≥rica - conjunto muy variado"
        elif category_diversity > 0.4:
            analysis += "- üìä Diversidad categ√≥rica moderada - buena variedad"
        else:
            analysis += "- üéØ Baja diversidad categ√≥rica - conjunto especializado"

        if topic_diversity > 0.6:
            analysis += "\n- ‚úÖ Alta diversidad tem√°tica - amplio rango de temas"
        elif topic_diversity > 0.3:
            analysis += "\n- üìä Diversidad tem√°tica moderada - temas relacionados"
        else:
            analysis += "\n- üéØ Baja diversidad tem√°tica - enfoque espec√≠fico"

        return analysis

    async def classify_by_custom_categories(
        self, documents: List[Document], custom_categories: List[str]
    ) -> Dict[str, List[str]]:
        """Clasificar documentos usando categor√≠as personalizadas"""

        # Agrupar por archivo
        files_content = defaultdict(list)
        for doc in documents:
            source = doc.metadata.get("source_file", "unknown")
            files_content[source].append(doc.page_content)

        # Clasificar cada archivo seg√∫n categor√≠as personalizadas
        results = defaultdict(list)

        for filename, chunks in files_content.items():
            combined_text = " ".join(chunks)

            prompt = f"""
            Clasifica el siguiente documento seg√∫n estas categor√≠as espec√≠ficas: {", ".join(custom_categories)}
            
            **Documento**: {filename}
            **Contenido**: {combined_text[:1000]}...
            
            Instrucciones:
            - Selecciona UNA categor√≠a principal de la lista proporcionada
            - Si no encaja en ninguna, indica "Otro"
            - Justifica brevemente tu elecci√≥n
            
            Categor√≠a seleccionada:
            """

            response = llm_manager.generate_response(prompt)

            # Extraer categor√≠a de la respuesta
            for category in custom_categories:
                if category.lower() in response.lower():
                    results[category].append(filename)
                    break
            else:
                results["Otro"].append(filename)

        return dict(results)

    async def generate_topic_hierarchy(self, documents: List[Document]) -> str:
        """Generar jerarqu√≠a de temas encontrados"""

        # Extraer todos los textos
        all_texts = [doc.page_content for doc in documents]
        combined_text = " ".join(all_texts)

        prompt = f"""
        Analiza el siguiente conjunto de textos y genera una jerarqu√≠a de temas organizados:
        
        {combined_text[:2000]}...
        
        Crea una estructura jer√°rquica de temas con el formato:
        
        # TEMA PRINCIPAL 1
        ## Subtema 1.1
        ### Detalle 1.1.1
        ### Detalle 1.1.2
        ## Subtema 1.2
        
        # TEMA PRINCIPAL 2
        ## Subtema 2.1
        
        Instrucciones:
        - M√°ximo 4 temas principales
        - 2-3 subtemas por tema principal
        - Detalles espec√≠ficos cuando sea relevante
        
        Jerarqu√≠a de temas:
        """

        return llm_manager.generate_response(prompt)

    def get_classification_stats(
        self, classifications: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Obtener estad√≠sticas de clasificaci√≥n"""

        if not classifications:
            return {"error": "No classifications available"}

        # Contar categor√≠as
        categories = [
            c["llm_classification"]["primary_category"]
            for c in classifications.values()
        ]
        category_counts = Counter(categories)

        # Contar niveles de especializaci√≥n
        levels = [
            c["llm_classification"]["specialization_level"]
            for c in classifications.values()
        ]
        level_counts = Counter(levels)

        # Contar palabras clave √∫nicas
        all_keywords = []
        for c in classifications.values():
            all_keywords.extend(c["llm_classification"]["keywords"])
        unique_keywords = len(set(all_keywords))

        return {
            "total_documents": len(classifications),
            "unique_categories": len(category_counts),
            "category_distribution": dict(category_counts),
            "specialization_levels": dict(level_counts),
            "unique_keywords": unique_keywords,
            "most_common_keywords": dict(Counter(all_keywords).most_common(10)),
        }
